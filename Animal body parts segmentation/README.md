
# Animal body parts segmentation

this was one of the problems in NUAI Hackathon and the challenge is providing an approach with high iou as a metric for this segmentation.
me and my team won the 3rd place in this competition and this is our approach.


## Data Augmentation

We used Albumentations library for augmentation. It is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance, and does so while providing a concise, yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection.


```bash
  import albumentations as A

def Album_batchtfms(): 
    #TODO : Remove bad augmentations by trial and error
    return A.Compose([
    A.CoarseDropout(p=0.3),
    A.GaussNoise(),
    A.RandomBrightnessContrast(p=0.3),
    A.RandomGamma(p=0.3),
    A.CLAHE(p=0.3),
    A.Flip(0.3)
])

```


## Fastai
We used Fastai library. it is an open-source deep learning library built on top of PyTorch that aims to make it easier to train and deploy machine learning models.

```bash
    from fastai.data.block import DataBlock
     dblock = DataBlock((ImageBlock,MaskBlock(codes)),
                get_items=pass_index,
                get_x = get_x,
                get_y=get_y,
                splitter=RandomSplitter(valid_pct=0.3),
                item_tfms=[Resize((350,350) , method='squish')],
                batch_tfms = AlbumentationsTransform(Album_batchtfms()))
        
    num_images = len(image)
    dls = dblock.dataloaders(list(range(num_images)) , bs=bs )


```
## Model selection

We used Unet with resnet34 as a backbone architecture. we tried using Efficientnet , convext but got low accuracy and low iou .we also tried resnet50 but it was more complex than resnet34 and taked alot of time for training


```bash
from timm.models.resnet import resnet34
learn = unet_learner(dls, resnet34, metrics=[IoU_f , acc_camvid], self_attention = True, act_cls = Mish, opt_func=opt , loss_func = CrossEntropyLossFlat(weight=weights, axis=1) ).to_fp16()

```
## Self Attention

self attention is one of the important techniques that we used for better performance. 

The network consists of three branches:
- Global branch : processing the entire image and determining the cropping ROI,
- Local branch : exhibiting the attention mechanism and processing the cropped image,
- Fusion branch : concatenating the pooling outputs of the global and local branches and performing final classification using dense layers.
The global branch is used to generate the heatmap which determines the cropped region. The heatmap is generated by counting the maximum values along channels at a certain high-level layer. Then a mask is generated with the same size as the heatmap. If the value of the per-channel maximized heatmap at a certain position (x,y) is larger than some threshold, the mask is assigned 1 at that position. Otherwise, the mask will have the value 0. Afterwards, the cropping area is determined so that all the points with mask value 1 are inside the cropping. Then the cropped part of the image is run through the local branch. Additionally, the outputs of both branches are fused in the fusion branch to perform an additional classification.

![self](https://user-images.githubusercontent.com/95244602/220905878-848deedc-b188-4b67-b7c9-38f007b9fcc3.jpg)



## Automatic learning rate finder

Learning rate finder plots lr vs loss relationship for a Learner. The idea is to reduce the amount of guesswork on picking a good starting learning rate.


```bash
learn.lr_find()

```
![lr](https://user-images.githubusercontent.com/95244602/220906726-d2a87fb1-c08b-4a91-836e-1c83dac40374.png)

## Gradients accumlation

We used gradients accumlations technique for spliting the batch of samples which are used for training a neural network into several mini-batches of samples that will be run sequentially.

![ga](https://user-images.githubusercontent.com/95244602/220907067-3cf602cf-c8be-4280-85fe-13afbcd51475.png)

Gradient accumulation means running a configured number of steps without updating the model variables while accumulating the gradients of those steps and then using the accumulated gradients to compute the variable updates.

![eg](https://user-images.githubusercontent.com/95244602/220906999-47ce6d76-660f-43d0-8f8c-8d389e20029a.png)

![-g](https://user-images.githubusercontent.com/95244602/220907028-20c0d3a6-d7e8-44a5-b21c-5e99981f59e7.png)


```bash
cbs = [
      GradientAccumulation(128),
      SaveModelCallback(monitor = "IoU_f" ),
      ]

lr = 1e-3
learn.fit_flat_cos(8, slice(lr) , cbs= cbs)
```
## Some results of our fine-tuned model 
![res1](https://user-images.githubusercontent.com/95244602/220905987-4d4a5535-9b6e-459a-a0e5-1135e27a163f.png)
![res 2](https://user-images.githubusercontent.com/95244602/220906284-19b2f433-24e9-451b-b8fe-33b0a75b8891.png)
